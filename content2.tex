
\chapter{Metodologia badań}
Niniejszy rozdział będzie opisywał etapy badań. W pierwszej kolejności wymienione zostaną narzędzia. Następnie, przedstawione będą elementy procesu eksploracji danych nie dotyczące pracy z algorytmami uczenia maszynowego, takie jak: wyszukiwanie, zbieranie, ekstrakcja cech, czyszczenie i normalizacja danych. Nawiązując do ankiety przeprowadzonej przez CrowdFlower: podczas badań związanych z {\em Big Data}, znacząca większość czasu pochłaniana jest przez zbieranie i przygotowanie danych[link]. Zasada ta miała swoje odzwierciedlenie również w tej pracy, szczególnie, że przedmiotem badań było wideo, które wymaga wykorzystania dużych zasobów podczas tych procesów (w odróżnieniu od na przykład danych tekstowych). Na końcu opisany zostanie proces trenowania, wyniki predykcji z modeli powstałych w oparciu o algorytmy wymienione z poprzedniej sekcji.
%https://visit.figure-eight.com/rs/416-ZBE-142/images/CrowdFlower_DataScienceReport.pdf -> slajd 5
\label{cha:pierwszyDokument}

\section{Narzędzia}
Specyfika badań wymagała narzędzi będących w stanie, sprawnie przetwarzać duże ilości informacji oraz zautomatyzować procesy związane z pobieraniem i przygotowaniem danych. Do tych zadań, jako optymalne rozwiązanie, został wybrany język programowania Python, według badań[link] jeden z najpopularniejszych języków programowania w {\em Data science}. Python pozwala użytkownikowi na korzystanie z zalet programowania strukturalnego, w przypadku prostych skryptów wykonujących krótkie zadanie (przykładowo: automatyczne kopiowanie i zmienianie nazw w paczkach plików), jak i obiektowego, gdzie konieczne jest ustalenie i ustrukturyzowanie pewnych zależności pomiędzy danymi. Dodatkową jego zaletą jest szereg bibliotek stworzonych z myślą o eksploracji danych. Do badań zostały wykorzystane następujące pakiety: sklearn, pandas, selenium (framework), numpy, matplotlib.\par\par
Poniżej wymienione narzędzia posłużyły do ekstrakcji danych z plików wideo:
\begin{itemize}[label=$\bullet$]
\item ffmpg -- jest aplikacją na licencji {\em opensource}. Dzięki niej otrzymane zostały informacje takie jak: rozdzielczość, dane o próbkowaniu chrominancji, liczba klatek na sekundę. Dodatkowo dzięki ffmpeg skompresowane pliki wideo rozstały rozłożone na surowy format YUV.
\item {\em AGH Video Quality Indicators} -- jest ogólnodostępnym oprogramowaniem rozwijanym przez Katedrę Telekomunikacji AGH, przy jego użyciu otrzymane zostały dane o metrykach typu {\em no-reference}[link].
\item VMAF Development Kit (VDK) -- podobnie jak powyższe VDK jest ogólnodostępnym oprogramowaniem rozwijanym przez Netflix. Zaimplementowane tu zostały algorytmy liczące metryki {\em full-reference}[link].
\end{itemize}
Zbieranie danych i wydobywanie cech wideo odbywało się w dwóch środowiskach: na lokalnie działającej maszynie wirtualnej z systemem operacyjnym Xubuntu oraz na super komputerze Prometeusz[link do podziękowań, czy coś w tym stylu] z systemem Centos. 

\label{cha:drugiDokument}



\section{Dane}
\label{cha:drugiDokument}

Na rysunku \ref{fig:data_preparation_work_flow} został przedstawiony schemat etapów przygotowywania danych. W celu dokonania obliczeń na dużych danych praktycznie każda z czynności dotycząca przetwarzania ich została zautomatyzowana. Kolejne akapity stanowią opis jego elementów schematu \ref{fig:data_preparation_work_flow}.



\begin{center}
\includegraphics[ height=11cm, width=15cm]{data_preparation_work_flow}
\captionof{figure}{Schemat kroków wykonanych podczas przygotowywania danych.}
\label{fig:data_preparation_work_flow}
\label{fig:xccs}
\end{center}

\subsection{Zbieranie danych}
Rozpoczęcie przetwarzania danych wymaga w pierwszej kolejności ich zebrania. Dla stworzenia algorytmu oceny jakości, podczas wyszukiwania danych, głównym kryterium było odnalezienie takich baz, gdzie oprócz plików wideo (SRC + PVS) istniała również jego subiektywna ocena. Ocena ludzka była konieczna, ponieważ to ona stanowiła dane nadzorcze podczas trenowania modeli. W pracy tej użyto następujące źródła danych:
\begin{itemize}
\item Subjective quality of H.264/SVC videos using ACR-HR method in VGA -- baza danych udostępniona dzięki The Institut de Recherche en Communications et Cybernétique de Nantes. W jej skład wchodzi około 300 plików wideo, ich czas trwania to: 10-12s, a rozdzielczość wszystkich wynosi: 640x480. \cite{pitrey:hal-00608310}.
\item Lab for Video and Image Analysis – LFOVIA -- jest grupą badaczy z Indian Institute of Technology Hyderabad zajmujących się tematyką jakości wideo. Dzięki udostępnionym przez nich plikom,  baza danych wejściowych w niniejszej pracy powiększyła się o 40 nowych obrazów w wysokich rozdzielczościach: FHD (1920x1080), UHD (3840x2160) i czasie trwania po 120 sekund każdy\cite{india}.
\item LIVE Netflix Video Quality of Experience Database -- baza danych stworzona przez naukowców z The University of Texas at Austin. W jej skład wchodzi 26 wideo o rozdzielczości 1920x1080, o długości 120 sekund każdy \cite{netflix_1}\cite{netflix_11}.
\item LIVE-NFLX-II Subjective Video QoE Database -- również stworzona przez The University of Texas at Austin w celu badań nad optymalnym przesyłem wideo w sieci. W bazie danych znajduje się 420 plików wideo o rozdzielczości FHD i czasie trwania około 40 sekund.
\item EPFL-PoliMI video quality assessment database -- Baza plików wideo utworzona przez dwie współpracujące uczelnie: Politecnico di Milano - Włochy i Ecole Polytechnique Fédérale de Lausanne - Szwajcaria. Dane składają się z 12 obrazów referencyjnych (SRC) oraz z aplikacji pozwalającej wygenerować dla nich PVSs. W ten sposób zostały uzyskane około 150 nowych filmów do badań w dwóch rozdzielczościach: 704x576, 352x288 i o czasie trwania około 10s\cite{italy}\cite{italy_2}\cite{italy_3}.
\end{itemize}
Początkowo w badaniach brała udział jeszcze jedna baza wideo z 220-oma plikami. Niestety, jak się okazało plik referencyjny oraz jego zniekształcone wersje posiadały różne wartości fps, przez co metryki typu VMAF nie były w stanie podać miarodajnych wyników. Cała więc baza musiała zostać pominięta.\par

\subsection{Ekstrakcja cech}
Kolejnym krokiem po zebraniu danych była ekstrakcja cech wideo, które później miały być użyte podczas przygotowywania modeli. W celu otrzymania cech należało każde wideo przekształcić do formatu surowego, nieskompresowanego YUV. Dalszy proces ekstrakcji był o tyle problematyczny, że różne, do tego wykorzystywane narzędzia, wymagały w różny sposób dostosowanych danych. Co więcej wideo bez kompresji zajmuje bardzo dużo przestrzeni dyskowej, a sam proces przetwarzania wideo wymaga również znacznego nakładu pracy dla procesora. Cechy biorące udział w badaniach to: blokowość, aktywność przestrzenna, aktywność czasowa, letterbox, pillarbox, straty bloków, rozmycie, wyciemnienie, zamrożenie, ekspozycja, kontrast, jasność, szum, PSNR, SSIM, MS-SSIM, VMAF, czas trwania, próbkowanie chrominancji, liczba klatek na sekundę.\par

Dane dotyczące metryk $full$-$renerence$ oraz $no$-$reference$ zostały uzyskane na każdą z ramek. W następnym kroku wartości te zostały uśrednione na całe wideo. Cechy pochodzące z obu źródeł: VDK i AGH Video Quality Indicators zostały ujednolicone i zapisywanym razem w postaci pliku CSV.\par

Dodatkowo w badaniu postanowiono dodać jeszcze jedną cechę, która mogła by mieć wpływ na wynik subiektywny, to znaczy ilość różnych rozdzielczości w bazie. Przykładowo w zestawieniu, w którym pliki posiadają różne rozdzielczości, te z ich wyższą wartością (np. FHD) mogą mocniej kontrastować z tymi o niższej, przez to ocena subiektywna może okazać wyższa. W pracy jej rozpoznano dwa przypadki, kiedy baza danych zawiera pliki o jednej rozdzielczości i baza danych zawiera pliki o dwóch rozdzielczościach. Cecha ta została wprowadzona jako binarna cecha nominalna dla każdego z wideo i odpowiada na pytanie czy plik pochodzi z bazy o jednej rozdzielczości. Jeżeli tak to wartość jej przyjmuje 1, w innym wypadku 0.  \par

Powyższy czynnik jest sztucznie stworzoną cechą, w komercyjnym wykorzystaniu trudną do otrzymania, ponieważ wymagało by to dostępu do obrazów obejrzanych przez widza bezpośrednio wcześniej. Cechę tę można również rozwinąć o inne atrybuty wideo, tak aby móc w lepszy sposób przedstawić różnice pomiędzy ocenianym plikiem, a pozostałą resztą, co leży poza zakresem opracowania.\par

Jako dane nadzorujące użyto subiektywną ocenę ludzką.\par

\subsection{Normalizacja i czyszczenie danych}
W zbiorze plików pochodzących od badaczy z Uniwersytetu w Texasie ocena subiektywna została przekazana po znormalizowaniu $z$-$score$, gdzie zakres wartości zawiera się pomiędzy -3 a 3. W celu otrzymania miarodajnych wyników dane z pozostałych baz również zostały w ten sam sposób znormalizowane.\par
Podczas procesu przygotowywania danych jak i późniejszej ekstrakcji cech mogły nastąpić pewne wyjątki, które uniemożliwiły otrzymanie poprawnego wyniku. W takim przypadku cały wiersz dotyczący danego wideo był pomijany podczas tworzenia modeli.

\subsection{Przygotowanie formatu danych pod modelowanie}

Przy tworzeniu modeli została wykorzystana implementacja zawarta w module języka Python sklearn. Instancje klas, przedstawiających algorytmy, przyjmują jako dane wejściowe dane w formacie macierzy. Dlatego pliki CSV zostały, podczas działania skryptu, przekształcone do typu DataFarme. Wspierany jest on przez moduł, ułatwiający prace z dużymi danymi -- pandas.\par

Przy eksploatowaniu danych często stosowaną techniką jest $ang.$ $cross$ $validation$. Polega ona na podziale dostępnych danych na dane treningowe i testowe. Można tu wykorzystać zasadę podziału 80:20, co oznacza ze 80 części danych jest przeznaczonych na dane treningowe, a 20 na dane testujące. Dobrą praktyką jest, aby w obu z tych grup znalazły się dane z całego przekroju bazy danych. Skutkuje to ograniczeniem problemu generalizacji czy nad dopasowania. W przedstawionej pracy proces podziału danych należało przeprowadzić, tak aby uwzględnić  całe grupy SRC i jego PSV. Dane z jednej takiej grupy powinny w całości znaleźć się w podzbiorze danych testowych bądź treningowych. Schemat podziału jest przedstawiony na rysunku \ref{fig:podzial_danych}.

\begin{center}
\includegraphics[ height=5cm, width=10cm]{podzial_danych}
\captionof{figure}{Przykładowy podział danych na dane testowe i treningowe (80:20). Niebieskie prostokąty odnoszą się do danych treningowych, a zielone do danych testowych.}
\label{fig:podzial_danych}
\label{fig:xccs}
\end{center}
Ostatnim etapem podczas przygotowywania danych było połączanie elementów z każdej z grup. Od tego momentu wektor danych wejściowych podczas przygotowywania modelu odnosi się do pojedynczego pliku wideo.




\subsection{Podsumowanie danych}
Poniższa tabela \ref{tab:tabela} przedstawia uzyskane dane 


\begin{table}[!htbp]
\centering
\begin{tabular}{|c|l|l|l|l|}
\hline
\multicolumn{5}{|c|}{5 baz danych, ponad 800 wektorów danych, ponad 100 paczek plików wideo} \\ \hline
\multicolumn{5}{|c|}{21 cech statystycznych} \\ \hline
\multicolumn{2}{|c|}{ilościowe} & \multicolumn{3}{c|}{jakościowe} \\ \hline
\multicolumn{2}{|l|}{\begin{tabular}[c]{@{}l@{}}blokowość, aktywność przestrzenna, aktywność czasowa,\\ letterbox, pillarbox, straty bloków, rozmycie, wyciemnienie, \\ zamrożenie, ekspozycja, kontrast, jasność, szum, \\ PSNR, SSIM, MS-SSIM, VMAF, czas trwania,\end{tabular}} & \multicolumn{3}{l|}{\begin{tabular}[c]{@{}l@{}}próbkowanie chrominancji,\\ rozdzielczość, \\ liczba dostępnych rozdzielczości\end{tabular}} \\ \hline
\end{tabular}
\caption{Podsumowanie przygotowania danych}
\label{tab:tabela}
\end{table}



\section{Modele }
\label{cha:drugiDokument}


Z uwagi na specyfikę przygotowania danych treningowych i testowych każdy z modeli został zbudowany i przetestowany 100 razy. Podział danych 80:20 dotyczy paczek SRC + PSV, natomiast budowa modeli odbywa się już na podstawie pojedynczych plików wideo. Dlatego może zaistnieć sytuacja kiedy większość małych paczek (na przykład po 3 pliki wideo) znajdą się w danych treningowych, a dużych (na przykład po 15 wideo) w danych testujących. Tym samym faktyczny podział może wynosić w skrajnym przypadku 50:50, zamiast 80:20 (taki sam odstęp może wystąpić w drugą stronę). Dlatego  przy  każdym uruchomieniu aplikacji, zajmującej się przygotowaniem danych  i tworzeniem modeli, podział zgromadzonych danych dobywał się na zasadzie losowo wybranych paczek do każdej grup, przy zachowaniu podziału 80:20. Następnie wyniki z przeprowadzonych uruchomień programu zostały uśrednione.\par
Uruchomienia sekwencji budowania i testowania odbywały się na następujących zestawach cech wideo \ref{tab:tabela2}. Dalsze rozdziały będą odnosić się do powyższej numeracji w kontekście zestawów cech.


\begin{table}[]
	\raggedright
	\begin{tabular}{|l|l|}
		\hline
		\textbf{\begin{tabular}[c]{@{}l@{}}Numer \\ zestawu\end{tabular}} & \textbf{Cechy}                                                                                                                                                                                                                                                                 \\ \hline
		1                                                                 & VMAF                                                                                                                                                                                                                                                                           \\ \hline
		2                                                                 & VMAF, SSIM                                                                                                                                                                                                                                                                     \\ \hline
		3                                                                 & MS-SSIM                                                                                                                                                                                                                                                                        \\ \hline
		4                                                                 & PSNR                                                                                                                                                                                                                                                                           \\ \hline
		5                                                                 & PSNR, VMAF                                                                                                                                                                                                                                                                     \\ \hline
		6                                                                 & PSNR, VMAF, SSIM, MS-SSIM                                                                                                                                                                                                                                                      \\ \hline
		7                                                                 & \begin{tabular}[c]{@{}l@{}}PSNR, VMAF, SSIM, MS-SSIM, blokowość, aktywność przestrzenna, pillarbox, \\ straty bloków, rozmycie, aktywność czasowa, wyciemnienie, ekspozycja, kontrast, \\ jasność, czas trwania, rozdzielczości\end{tabular}                                   \\ \hline
		8                                                                 & \begin{tabular}[c]{@{}l@{}}PSNR, VMAF, SSIM, MS-SSIM, blokowość, aktywność przestrzenna, pillarbox, straty bloków, \\ rozmycie, aktywność czasowa, wyciemnienie, ekspozycja, kontrast, jasność, czas trwania, rozdzielczości, \\ liczba dostępnych rozdzielczości\end{tabular} \\ \hline
		9                                                                 & PSNR, VMAF, SSIM, MS-SSIM, rozmycie, straty bloków                                                                                                                                                                                                                             \\ \hline
		10                                                                & PSNR, VMAF, SSIM, MS-SSIM, rozmycie, straty bloków, liczba dostępnych rozdzielczości                                                                                                                                                                                           \\ \hline
		11                                                                & PSNR, VMAF, SSIM, MS-SSIM, jasność                                                                                                                                                                                                                                             \\ \hline
		12                                                                & PSNR, VMAF, SSIM, MS-SSIM, jasność, blokowość, straty bloków, rozmycie                                                                                                                                                                                                         \\ \hline
		13                                                                & \begin{tabular}[c]{@{}l@{}}PSNR, VMAF, SSIM, MS-SSIM, jasność, blokowość, straty bloków, rozmycie, \\ liczba dostępnych rozdzielczości\end{tabular}                                                                                                                            \\ \hline
		14                                                                & PSNR, VMAF, SSIM, MS-SSIM, jasność, ekspozycja                                                                                                                                                                                                                                 \\ \hline
		15                                                                & PSNR, VMAF, SSIM, MS-SSIM, jasność, ekspozycja, liczba dostępnych rozdzielczości                                                                                                                                                                                               \\ \hline
		16                                                                & PSNR, VMAF, SSIM, MS-SSIM, jasność, liczba dostępnych rozdzielczości                                                                                                                                                                                                           \\ \hline
		17                                                                & PSNR, VMAF, SSIM, MS-SSIM, czas trwania                                                                                                                                                                                                                                        \\ \hline
		18                                                                & PSNR, VMAF, SSIM, MS-SSIM, czas trwania, liczba dostępnych rozdzielczości                                                                                                                                                                                                      \\ \hline
		19                                                                & \begin{tabular}[c]{@{}l@{}}PSNR, VMAF, SSIM, MS-SSIM, czas trwania, liczba dostępnych rozdzielczości,\\  rozdzielczości\end{tabular}                                                                                                                                           \\ \hline
		20                                                                & PSNR, SSIM                                                                                                                                                                                                                                                                     \\ \hline
		21                                                                & SSIM                                                                                                                                                                                                                                                                           \\ \hline
		22                                                                & blokowość                                                                                                                                                                                                                                                                      \\ \hline
		23                                                                & \begin{tabular}[c]{@{}l@{}}blokowość, aktywność przestrzenna, pillarbox, straty bloków, rozmycie, aktywność czasowa,\\  wyciemnienie, ekspozycja, kontrast, jasność, rozdzielczości, liczba dostępnych rozdzielczości\end{tabular}                                             \\ \hline
		24                                                                & \begin{tabular}[c]{@{}l@{}}blokowość, aktywność przestrzenna, pillarbox, straty bloków, rozmycie, aktywność czasowa, \\ wyciemnienie, ekspozycja, kontrast, jasność, czas trwania, rozdzielczości\end{tabular}                                                                 \\ \hline
		25                                                                & \begin{tabular}[c]{@{}l@{}}blokowość, aktywność przestrzenna, pillarbox, straty bloków, rozmycie, aktywność czasowa,\\ wyciemnienie, ekspozycja,  kontrast, jasność, czas trwania, rozdzielczości, liczba dostępnych rozdzielczości\end{tabular}                               \\ \hline
		26                                                                & straty bloków                                                                                                                                                                                                                                                                  \\ \hline
		27                                                                & rozmycie                                                                                                                                                                                                                                                                       \\ \hline
		28                                                                & jasność                                                                                                                                                                                                                                                                        \\ \hline
		29                                                                & kontrast                                                                                                                                                                                                                                                                       \\ \hline
		30                                                                & ekspozycja                                                                                                                                                                                                                                                                     \\ \hline
	\end{tabular}
\label{tab:tabela2}
\end{table}



W niniejszej pracy zostały przetestowane cztery algorytmy uczenia maszynowego przedstawione w części teoretycznej. Parametry definiujące ich działanie zostały dobrane na zasadzie obserwacji kolejnych prób i wiedzy na temat ich działania. W ten sposób zostały wyselekcjonowane poniższe parametry, które pozwoliły na osiągnięcie najwyższego wskaźnika R-kwadrat dla jednego z zestawów.  \par

Regresja liniowa została sprawdzona w pierwszej kolejności. Jako jeden prostszych z algorytmów, nie wymaga ona podania dodatkowych parametrów, aby zadziałał w prawidłowy sposób. W tym przypadku wektor danych wejściowych podczas trenowania przedstawia się  następująco \ref{eqn:regresja}, gdzie $y_i$ jest subiektywną oceną dla filmu $i$, a $x_{1i}, x_{2i}...$ są jego cechami. \par

Kolejnym testowanym algorytmem był SVR. Użyta funkcja kernela do wyznaczenia hiperpłaszczyzny to $Radial$ $Basis$ $Function$ (RBF). Współczynnikowi $\epsilon$ została przypisana wartość $0,1$. Przy współczynniku $\gamma$, która mówi jak odległe punkty będą miały wpływ na budowaną hiperpłaszczyznę, skorzystano z metody już zaimplementowanej w module sklearn, pozwalającej na wyznaczenie w bardziej dopasowane wartości, bazując na dostępnym zbiorze danych. Parametr $C$, który odpowiada za gładkość funkcji SVR wynosi 0.7.\par
%https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2839056/

Las losowy działa w oparciu o parametry podane przez badacza, takie jakie jak ilość drzew i ich głębokość. W tym przypadku użyto 100 drzew o głębokości 8. Jako metodę podejmowania decyzji o podziale danych (tworzenia węzłów decyzyjnych), został wybrany \emph{Gini Index} \par

Ostatnim sprawdzonym algorytmem uczenia maszynowego była sieć neuronowa z następującymi parametrami -- 3 warstw ukrytych po 13 neuronów w każdej. Maksymalna ilość iteracji algorytmu $Backward$ $Propagation$ po której model będzie gotowy to 1000. Parametr $learning$ $rate$, określający szybkość uczenia, został ustawiony na wartość 0,001.\par


\chapter{Wyniki i analiza }
Poniższy rozdział prezentuje wyniki z poszczególnych modeli. Zostały one zebrane na wykresach słupkowych, gdzie os x odnosi się do numerów zestawów, a os y do średnich wartości R-kwadrat \ref{fig:podsumowanie}. Wykres ten przedstawia wyniki dla ostatecznie wybranych parametrów algorytmów. Selekcja dokonana została na podstawie najlepszego osiągniętego wyniki dla jednego z zestawów na rodzaj algorytmu. Przykładowo dla  sieci neuronowych testowane były konfiguracje: 2 warstwy wykryte i 8 neuronów, 4 warstwy ukryte 13 neuronów, 3 warstwy ukryte 13 neuronów.\todo[inline]{tak naprawde duzo wiecej ale chyba nie ma sensu tego pisac, bo niektore testy to na zasadzie kolejnego uruchamiania i obserwacji robione} Każda z nich była testowana na każdym wcześniej zdefiniowanym zestawie. Najwyższy R-kwadrat został osiągnięty dla ostatniej z wymienionych konfiguracji i dla zestawu 19. Tak więc prezentowane wyniki z tabeli \ref{fig:podsumowanie} będą odnosić się do tych właśnie parametrów, ale dla każdego z zestawów. Czego wynikiem mogą pewne negatywne oddziaływania związane z przetwarzaniem danych dla ......[dokonczyc ten akapit]   
W dalszej części rozdziału  odbędzie się ich analiza.


\begin{sidewaysfigure}[ht]
\includegraphics[ height=15cm, width=26cm]{podsumowanie}
\captionof{figure}{Wyniki z poszczególnych modeli}
\label{fig:podsumowanie}
\label{fig:xccs}
\end{sidewaysfigure}

W pierwszej kolejności wyjaśniane zostaną różnice w obrębie zestawów danych, a pomiędzy poszczególnymi modelami. Różnice te wynikać mogą z charakterystyki testowanych algorytmów uczenia maszynowego. Niektóre z nich potrafią odnaleźć fałszywe zależności w danych treningowych, które nie wystąpiły w czasie testów, czyli występuje  nad-dopasowanie dla pewnych cech. Inną przyczyną może być generalizacja, przez którą może nastąpić pominięcie pewnych informacji ukrytych w danych.\par

Obserwując poszczególne wyniki dla zestawów $no$-$reference$ (22 -- 30)  oraz $full$-$reference$ (1 -- 6, 20, 21) można zaobserwować, że te drugie znacznie trafniej dokonują predykcji. Prawdopodobną przyczyną występowania tej sytuacji jest nierównomierność w występowaniu cechy typu NR w dostępnych danych, co może dalej skutkować nadpasowanie modelu. 

Co  więcej algorytm wykazał że wprowadzona dodatkowa cecha (liczba dostępnych rozdzielczości) również ma  wpływ na ocenę jakość, choć w niewielkim stopniu.  W większości przypadków zestaw z tą cecha otrzymywał wyższe R-kwadrat, niż ten sam zestaw ale bez liczby dostępnych zależności. Wykres kołowy x przedstawia w jakiej części przypadków dana obserwacja się powtarza  \ref{tab:tabela rozdzielczosc}. Podobne, sprawdzone tu, oddziaływanie to czas trwania. Dla dłuższych wideo  model daje lepsze wyniki, różnice również są nieznaczne. \par

\begin{center}
\includegraphics[ height=8cm, width=10cm]{w_kolowy}
\captionof{figure}{Wykres przedstawia stosunek liczby modeli zbudowanych z zestawów z dodatkową cechą, ocenionych jako bardziej trafnych, do liczby przypadków kiedy taki model został oceniony jako gorszy. W 40 przypadkach oceniony, jako lepszy 8 jako gorszy. Rozwarzane zestawy to pary: 14 i 15, 17 i 18, 7 i 8, 11 i 16, 9 i 10, 24 i 25.}
\label{tab:tabela rozdzielczosc}
\label{fig:xccs}
\end{center}
 
%zestawy: 14 i 15, 17 i 18, 7 i 8, 11 i 16, 9 i 10, 24 i 25 są dla siebie bliźniacze z różnicą dodatkowej cechy. Zestawy oznaczone * zawierają dodatkową ceche. Zielony kolor wskazuję na pole z większym R-kwadrat na pare, czerwony na mniejszy(test powtórzony dwa razy). 
  
Na wykresie \ref{fig:podsumowanie} można dostrzec, że różnice pomiędzy modelami zbudowanymi na podstawie zestawów xxx wyróżniają się pod kątem wysokich wartości R-kwadrat. Dla tych zestawów wspólnym mianownikiem są cechy: PSNR, VMAF, SSIM, MS-SSIM. Każda cecha z osobna również osiąga wysokie wyniki (Przykładowo, R-kwadrat kolejno dla modelu Lasu Losowego:0.490, 0.502,  0.475,  0.507). Niższe jednak niż te osiągnięte w zestawie. 
Wymienione wyżej cechy posiadają wysoki współczynnik korelacji \ref{fig:korelacja}, przez co łączenie ich tylko w pewnym stopniu poprawia trafność modelu. Końcowa wartość nie jest ich sumą. Przykładowo dla zestawu 2 (VMAF + SSIM) R-kwadrat wynosi 0.636 \par



\begin{center}
\includegraphics[ height=14cm, width=14cm]{korelacja}
\captionof{figure}{Korelacja pomiędzy poszczególnymi cechami.}
\label{fig:korelacja}
\label{fig:xccs}
\end{center}





Kolejny aspekt, który można odnotować czytając wykres, to ujemna wartość R-kwadrat dla niektórych zestawów. Biorąc pod uwagę wzór \ref{eqn:r2}, zauważyć należy to, że model podczas testów dokonywał gorszej predykcji, niż średnia dla zbioru danych testowych. Tą sytuację przedstawia wykres \ref{fig:kontrast_ujemne_rk}. Zaprezentowane są tu próbki danych dla cechy kontrast. Funkcja regresji liniowej (linia niebieska)  pokrywa się ze średnią dla danych testowych (linia pomarańczowa), ale podczas liczenia R-kwadrat brana pod uwagę jest średnia ze zbioru danych testowych (linia czerwona). Gdyby ta ostania pokrywała się z linią regresji R-kwadrat wynosił by 0. Ujemny, R-kwadrat oznacza również, że dla danej cechy lub ich zestawu, algorytm nie odnalazł zależności między zmiennymi opisującymi, a zmienną opisywaną.\par
\begin{center}
\includegraphics[ height=9cm, width=13cm]{kontrast_ujemne_rk}
\captionof{figure}{Przypadek kiedy wskaźnik r-kwadrat jest ujemny.}
\label{fig:kontrast_ujemne_rk}
\label{fig:xccs}
\end{center}

Najwyższy uśredniony wynik R-kwadrat został osiągnięty przez model zbudowany na podstawie algorytmu lasu losowego (z zadanymi wcześniej parametrami) z użyciem zestawu 18, czyli  0.824. Składa się on z  zebranych metryk full-reference z uwzględnieniem czasu trwania wideo i dodatkowej cechy dotyczącej ilości dostępnych rozdzielczości. Odnosząc się do tematu pracy, ma on największe szanse na trafne wyznaczenie jakość wideo. Konkretny model nie może być w tym przypadku odpowiedzią ponieważ bazuje się tu na podstawie uśrednionych wyników z ponad 100 wykonań programu. Każdy z modeli z tej grupy daje różne wyniki (odchylenie standardowe wynosi 0.053, jest stosunkowo nieduże w porównaniu z innymi modelami i zestawami) ponieważ budowany jest na różnych próbkach danych.


\chapter{Podsumowanie}


Prezentowana praca stawiała sobie za cel stworzenie algorytmu oceniającego jakość wideo.

Po uprzednim przygotowaniu danych, do dyspozycji badającego były informacje opisujące wideo –zarówno metryki FR jak i NR. Metryki zostały wykorzystane do zbudowania i przystosowania czterech modeli w oparciu o algorytmy uczenia maszynowego. Wykreowane modele mogą posłużyć jako algorytmy oceniające jakość wideo.

Zmierzając do zniwelowania różnic wynikających ze specyficznego podziału danych stworzono ponad 100 modeli dla każdego z algorytmów. Gdzie, na podstawie uśrednionego wyniku R-kwadrat, został wybrany optymalny algorytm (wraz z parametrami) i zestaw cech - jako dane wejściowe. Odnosząc się do tematu pracy, algorytmem oceniający jakość jest jeden z modeli przygotowany w oparciu o wyżej wymienione czynniki -- Las Losowy składający się z 100 drzew o głębokości 8 wraz cechami z zestawu 18 [link do zserializowanego modelu].

Co więcej, można postawić tezę, że chociaż wszystkie metryki FR, w tym badaniu, mają wysokie wartości R-kwadrat, to jednak możliwa jest sytuacja, kiedy ich ocena znacząco będzie odbiegać od SQV. Wideo referencyjne może już na samym początku zostać zniekształcone poprzez nieprawidłowe nagranie. Policzony PSNR wykaże wysoką jakość obrazu ale ocena odbiorcy będzie negatywna.

Przedstawione w obecnym studium rozwiązanie nie jest uniwersalnym wyznacznikiem jakości wideo. Z pewnością mogą istnieć czynniki, które nie zostały w pracy uwzględnione. Z faktu, że badanie korzysta z oceny ludzkiej jako cechy nadzorującej, mogą wynikać, nie brane pod uwagę efekty zmieniające tę ocenę.

Należy stwierdzić, że z biegiem lat oczekiwania co do jakości wideo znacznie rosną. Chociaż w latach 90 ubiegłego wieku tamtejsze wideo uznawane było jako produkt wysokiej klasy, to współcześni widzowie nie byliby w żaden sposób usatysfakcjonowani odbiorem takiego obrazu. 

Dodatkowo, biorąc dynamicznie rozwijający się rynek multimediów, warto zwrócić uwagę na nowe technologie jeszcze nie tak mocno spopularyzowane. Dla nich może się okazać, że żadna do tej pory dostępnych metryk, również ta  zaproponowana w niniejszej pracy,  nie spełnia oczekiwań. Przykładowo nie brany do tej pory, pod uwagę w czasie badań, rodzaj wideo wykorzystywana w VR -- wideo 360.  

Przeprowadzone badania i zebrane dane mogą posłużyć do dalszych bardziej
zaawansowanych studiów nad jakością wideo. Mogą zostać wykorzystane, do stworzenia metryki, która byłaby w stanie na bieżąco dostosowywać się i oceniać wideo w zależności od aktualnego rodzaju zniekształcenia obrazu.


\label{cha:pierwszyDokument}









